# 통계 이론

## 통계학이란?

- 데이터에 담겨진 표면적인 정보를 정확히 요약하고 그 내면에 담긴 의미를 추론하고 해석하기 위한 도구적인 성격의 학문

- 미지의 정보인 모수를 주어진 데이터를 이용해 맞추고자 한다. > 정확히 맞출 수 없기에 모수 예측에 오차를 가능한 한 축소시키는 것을 목표로 한다.

## 확률

### 확률의 기본 개념

- 확률: 어떤 사건이 발생할 가능성을 0에서 1사이의 숫자로 표현한 것

- 확률모형: 시행을 반복할 때마다 나오는 결과가 우연에 의존하여 매번 달라지는 현상 또는 실험(확률실험)에 대한 수리적 모형

- 표본공간: 확률 실험에서 모든 관찰 가능한 결과의 집합. S로 표기한다.<br>예: 동전을 던졌을 때 나오는 결과 S = {앞면, 뒷면}

- 사건: 표본공간의 임의의 부분집합. A, B 등으로 표기한다.

### 확률의 정의 및 성질

- 고전적 접근

    - n개의 실험결과로 구성된 표본공간에서 각 실험결과가 일어날 가능성이 같은 경우, m(m<=n)개의 실험결과로 구성된 사건 A의 확률을 아래와 같이 정의한다.
    
    $$ P(A) = \frac{M}{N} $$

- 상대적 비율에 의한 접근

    - n번의 반복된 실험 중 어떤 사건 A가 발생한 횟수를 m이라고 할 때, 사건 A의 상대빈도(상대도수)는 $frac{m}{n}$으로 구해진다.
    
    - 이 실험의 반복 횟수 n을 무한히 증가했을 때, 사건 A의 상대빈도가 수렴하는 값을 사건 A의 확률로 정의하고자 하는 접근이다.

- 확률의 공리적 정의

    1. 임의의 사건 A에 대해 P(A) >= 0
    
    2. P(S) = 1
    
    3. 표본공간 S에 정의된 서로 상호배반인 사건 $A_{1}, A_{2}, ...$에 대해 모든 사건의 합집합은 각 사건을 더한 결과와 같다.

- 공리적 접근방식: 표본공간을 정의역으로 하며, 위 세가지 공리를 만족하는 함수를 확률로 정의한다.

- 확률의 규칙

    - 여사건의 확률
    
        - $P[A^{C}]$: 사건 A를 제외한 나머지 사건의 확률
        
        - $P[A^{C}] = 1 - P[A]$
    
    - 곱사건의 확률
    
        - $P[A \cap B]$: 사건 A와 사건 B가 동시에 발생할 확률
    
    - 합사건의 확률
    
        - $P[A \cup B]$: 사건 A 또는 사건 B가 발생할 확률
        
        - $P[A \cup B] = P[A] + P[B] - P[A \cap B]$
    
- 조건부 확률과 독립

    - 조건부 확률: 사건 A와 B가 표본공간 S 상에 정의되어 있으며, P(B) > 0 라고 가정한다. 이 때 B가 일어났다는 가정 하의 사건 A가 일어날 조건부 확률은 아래 식과 같이 정의된다.
    
    $$ P(A|B) = \frac{P(A \cap B)}{P(B)} $$
    
    - 독립 사건: 두 사건 A와 B가 다음 중 하나를 만족시키면 서로 독립이라고 한다.(단, P(A) > 0, P(B) > 0)
    
        1. $P(A|B) = P(A)$
        
        2. $P(A \cap B) = P(A) * P(B)$
        
        3. $P(B|A) = P(B)$

### 베이즈 정리

#### 표본공간의 분할과 전확률 공식

- 표본공간의 분할

    - $B_{1}, ..., B_{k}$가 다음 조건을 만족하면 표본 공간 s의 분할이라고 한다.
    
        - 서로 다른 i, j에 대해 $B_{i} \cap B_{j} = \emptyset$
        
        - $B_{1} \cup B_{2} \cup ... \cup B_{k} = S$
    
    - k개의 원인이 있을 때 각각의 원인들이 만족해야 하는 조건이 분할이어야 한다.

- 전확률공식

    - 사건 $B_{1}, B_{2}, ..., B_{k}$는 상호배반이며, $B_{1} \cup ... \cup B_{k} = S$라고 함.
    
    - 이 때 S에서 정의되는 임의의 사건 A에 대하여 다음이 성립한다.
    
    $$ P(A) = P(A \cap B_{1}) + ... + P(A \cap B_{k})
            = P(B_{1})P(A|B_{1}) + ... + P(B_{k})P(A|B_{k})$$
    
    - 결과의 확률을 구할 때 어떤 원인의 확률과 그 원인을 전제로 했을 때 결과의 관계를 알면 결과의 확률을 알 수 있다.

#### 베이즈 정리

- 베이즈 정리

    - 사건 $B_{1}, B_{2}, ..., B_{k}$는 상호 배반이며, $B_{1} \cup ... \cup B_{k} = S$라고 함.<br>-> 분할이 만족해야 한다.
    
    - 이 때 사건 A가 일어났다는 조건 하에서 사건 $B_{i}$가 일어날 확률은 다음과 같다.
    
    $$ P(B_{i}|A) = \frac{P(A \cap B_{i})}{P(A)} = \frac{P(B_{i})P(A|B_{i}))}{P(B_{1})P(A|B_{1}) + ... + P(B_{k})P(A|B_{k})}$$
    
    - 베이즈 정리 활용: $B_{1}, B_{2}, ..., B_{k}$으로 분할된 사건의 각 확률을 알고, 각 $B_{i}$를 전제로 한 각 $B_{1}$의 조건부 확률을 구하기 위한 정리

### 확률과 확률분포

#### 확률변수

- 확률변수: 표본 공간에서 정의된 실수값 함수 > 사건의 결과를 실수로 바꿔준 것으로 볼 수 있다.

    - 이산형 확률변수: 확률변수가 취할 수 있는 값이 셀 수 있는 경우
    
    - 연속형 확률변수: 주어진 구간에서 모든 실수 값을 취할 수 있어 셀 수 없는 경우

- 확률분포

    - 확률질량함수(pmf): 확률변수 X가 이산형인 경우 X가 취할 수 있는 값 $x_{1}, x_{2}, ..., x_{n}$의 각각에 대하여 확률 $P[X = x{1}], P[X = x_{2}], ..., P[X = x_{n}]$을 대응시켜 주는 관계를 X의 확률질량함수라고 하며 $f(x)$로 표기한다.
    $$ f(x_{i}) = P=[X = x_{i}],   i = 1, 2, ..., n$$
    
    - 확률질량함수의 성질
    
        1. 모든 $i = 1, 2, ..., n$에 대해 $0 <= f(x_{i}) <= 1$
        
        2. $\sum_{i=1}^{n} f(x_{i}) = 1$

    - 확률밀도함수(pdf): 확률변수 X가 연속형인 경우 X가 가질 수 있는 구간 위에서의 함수 f(x)가 다음을 만족할 때, 이를 X의 확률밀도함수라고 함
    $$\int\limits_a^b f(x)dx = P[a \leq X \leq b] (단, -\infty < a < b < \infty)$$
    
    - 확률밀도함수의 성질
    
        1. 모든 a, b에 대해 $0 \leq \int\limits_a^b f(x)dx \leq 1$
        
        2. $\int\limits_\infty^\infty f(x)dx = 1$
    
    - 누적분포함수(cdf): X의 확률밀도함수가 $f(x)$일 때, X의 누적분포함수 $F(x)$는 $X \leq x$인 모든 X에 대한 $f(x)$의 적분 값이 됨. > 연속형, 이산형 모두 존재.
    
        - $F(-\infty) = 0, F(\infty) = 1$
        
        - x가 증가할 때 $F(x)$도 증가하며, $F(x)$는 음의 값을 가질 수 없음

- 확률분포의 특성치

    - 기대값: 분포의 무게중심, 중심 위치를 나타내며, $E(X) = \mu$로 표기한다.
    
        - 이산형: $\sum_{all x}^{} xf(x)$
        
        - 연속형: $\int\limits_\infty^\infty xf(x)dx$
    
    - 분산: 분포의 산포를 나타냄.
    $$V[X] = \sigma^{2} = E[(X - \mu)]$$
    
    - 표준편차: 분산의 제곱근. 단위가 보정된다.
    $$S[X] = \sigma = \sqrt{V[X]}$$

#### 확률분포

##### 1. 이항분포

- 베르누이 시행: 매 시행마다 '성공' 또는 '실패'의 오직 두 가지 가능한 결과만 가지고, '성공'의 확률이 p로 일정하다는 두 조건을 만족하는 실험

- 이항 확률변수가 고려되는 실험: 베르누이 시행을 독립적으로 n번 반복하는 실험이 해당

- 이항확률변수와 확률질량함수

    - X: n번 시행 중 '성공'의 횟수로 정의
    
    - x=0, 1, ..., n의 값을 가진다.
    
    - $f(x) = P(X = x) = A_{m,n} = \begin{pmatrix}  n \\  x \\ \end{pmatrix} p^{x}(1-p)^{n-x}$
    
    - 이 경우 $X \sim Bin[n, p]$라고 한다. > n, p를 알면 분포를 정의할 수 있음.

- 이항분포의 특성치

    - $X \sim Bin[n, p]$인 경우 $E[X] = np, V[X] = np(1-p)$를 만족한다.

##### 2. 포아송 분포

- 포아송 확률변수와 확률질량함수

    - 단위 시간에 (t=1), 포아송 확률과정을 따르는 사건 A가 발생하는 횟수를 X로 정의하면 아래와 같다.
    
    $$ f(x) = P(X = x) = \frac{exp(-m)m^{x}}{X!}, x = 0, 1, 2, ... $$
    
    - 이 경우 $X~POI[m]$라고 한다.

- 포아송 분포의 특성치

    - $X \sim POI[m]$인 경우, $E[X] = V[X] = m$을 만족한다.

##### 3. 지수분포

- 지수확률변수와 확률밀도함수

    - 단위 구간에서 평균발생횟수가 m인 포아송 확률과정을 따르는 사건 A가 한 번 일어난 뒤 그 다음 또 일어날 때까지 걸리는 시간 X로 정의됨.
    
    $$ f(x) = \frac{1}{\lambda}exp(-\frac{x}{\lambda}), x > 0임. \left(단, m = \frac{1}{\lambda}\right) $$
    
    - 이 경우  $X \sim EXP[\lambda]$ 라고 한다.

- 지수 분포의 특성치

    - 포아송 모수와 지수 모수는 역의 관계: 단위구간 내 평균 발생횟수가 m인 포아송 과정을 따르는 사건은 사건 사이 소요시간의 평균이 $\lambda = \frac{1}{m}$이다.
    
    - $X \sim EXP[\lambda]$인 경우, $E[X] = \lambda$를 만족한다.

##### 4. 감마분포

- 감마확률변수와 확률밀도함수

    - 감마확률변수 X의 확률밀도함수는 양수인 $\theta$와 k에 대하여 다음과 같이 정의된다.
    
    $$ f(x) = \frac{1}{\theta^k\gamma(k)}x^{k-1}exp(-x/\theta), x > 0 $$
    
    - 이 경우 $X \sim GAMMA[k, \theta]$라고 한다. > k는 shape를 결정하는 모수, $\theta$는 스케일(분포)를 결정하는 모수.

- 감마분포의 특성치

    - $X \sim GAMMA[k, \theta]$인 경우에 $E[X] = k\theta, V[X] = k\theta^{2}$

##### 5. 정규분포

- 정의

    - 확률변수 X가 평균이 $\mu$, 분산이 $\sigma^{2}$이고, 다음의 확률밀도함수를 가질 때, X는 정규분포를 따른다고 하며, 이 경우 $X \sim N[\mu, \sigma^{2}]$라고 한다.

    $$ f(x) = \frac{1}{\sqrt{2\pi}\sigma}e^{\frac{-(x-\mu)^{2}}{2\sigma^{2}}}, -\infty < x < \infty $$

- 정규분포 확률밀도함수의 개형

    - $\mu$는 분포의 중심.
    
    - $\mu$를 중심으로 대칭이고, $\mu$에서 가장 큰 값이 되는 하나의 봉우리만 가진다.
    
    - $\sigma^{2}$이 크면 분포의 산포가 커지고, 작으면 분포의 산포가 작아진다.

- 정규분포의 특성치

    - $X \sim N[\mu, \sigma^{2}]$인 경우, $E[X] = \mu, V[X] = \sigma^{2}$

##### 6. 표준정규분포

- 표준정규분포와 정규확률변수의 표준화

    - 표준정규분포
    
        - $X \sim N[\mu, \sigma^{2}]$일 때, 정규분포의 선형불변성에 의해 $Z = \frac{X-\mu}{\sigma} \sim N[0, 1]$이 되며, 이 때 평균이 0, 분산이 1인 정규분포를 표준정규분포라 정의한다.
        
        $$ f(z) = \frac{1}{\sqrt{2\pi}}e^{-x^{2}/2}, -\infty < z < \infty $$

    - 표준화: 정규확률분포를 $Z = \frac{X-\mu}{\sigma}$을 이용하여 표준정규분포로 변경할 수 있다. 이 과정을 표준화라고 한다.

    - 표준정규 확률변수의 (1-$\alpha$)분위수: $z_{\alpha}$
    
        - $Z \sim N[0, 1]$일 때, $P[Z < c] - 1 - \alpha$를 만족하는 Z의 $(1 - \alpha)$분위수 c를 $Z_{\alpha}$으로 표기한다.
        
        - $Z_{0.05} = 1.645, Z_{0.025} = 1.96$

##### 7. 카이제곱 분포

- 정의

    - $Z_{1}, Z_{2}, ..., Z_{k}가 k개의 서로 독립인 표준정규 확률변수 ($Z_{i} \sim N[0, 1], i = 1, 2, ..., k$)라고 할 때, $X = Z_{1}^{2}+Z_{2}^{2}+...+Z_{k}^{2}$가 따르는 분포를 자유도가 k인 카이제곱 분포라고 정의함. > 표준정규들의 제곱합이 가지는 분포
    
    $$ f(x) = \frac{1}{r\left(\frac{1}{2}\right)2^{\frac{k}{2}}}x^{\frac{k}{2}-1}e^{-\frac{x}{2}}, 0 < x < \infty$$
    
    - 이 경우 $X \sim x^{2}[k]$라고 함.

- 카이제곱 분포의 특성치

    - $X \sim x^{2}[k]$인 경우, $E[X] = k, V[X] = 2k$를 만족한다.

- 카이제곱 분포 확률밀도함수 개형: 오른쪽 꼬리가 길게 늘어진 비대칭 형태이다.

- 카이제곱 확률변수의 $(1 - \alpha)$분위수 : $X^{2}_{a, k}$

    - $X \sim x^{2}[k]$일 때, $P[X > c] = \alpha$를 만족하는 $X$의 $(1 - \alpha)$분위수 c를 $X^{2}_{a, k}$으로 표기한다.

##### 8. t 분포

- 정의

    - Z가 표준정규 확률변수 $Z \sim N[0, 1]$이라 할 때, U가 자유도가 k인 카이제곱 확률변수 $U \sim x^2[k]$이며, Z와 X는 서로 독립이라고 할 때, $X = \frac{z}{\sqrt{U/k}}$가 따르는 분포를 자유도가 k인 t분포라고 정의한다. > 표준정규의 평균이 가지는 분포
    
    $$ f(x) = \frac{r\left(\frac{k + 1}{2}{2}\right)}{r\left(\frac{k}{2}\right)}\frac{1}{\sqrt{kl\pi}}\frac{1}{\left(1 + \frac{x^2}{k}]\right)^{\frac{k + 1}{2}}}, -\infty < x < \infty$$
    
    - 이 경우 $X \sim t[k]$라고 한다.

- t 분포의 특성치

    - $X \sim t[k]$인 경우, $E[X] = 0, V[X] = \frac{k}{k-2}$(단, k > 2)를 만족한다.

- t분포 확률밀도함수 개형

    - $X \sim t[k]$인 경우
    
    1. 가운데 0을 중심으로 대칭인 종모양의 분포
    
    2. 표준정규분포보다 꼬리가 두껍다
    
    3. 자유도 k가 커짐에 따라 산포가 줄어들어 표준정규분포로 수렴한다.

- t 확률변수의 $(1 - \alpha)$분위수: $t_{a, k}$

    - $X \sim t[k]$일 때, $P[X > c] = \alpha$를 만족하는 X의 $(1 - \alpha)$분위수 c를 $t_{a, k}$으로 표기한다.

##### 9. F 분포

- 정의

    - U가 자유도가 $k_1$인 카이제곱 확률변수 $U \sim x^2[k_1]$이며, V가 자유도가 $k_2$인 카이제곱 확률변수 $V \sim x^2[k_2]$이고, U와 V는 서로 독립이라고 할 때, $X = \frac{U/k_1}{V/k_2}$가 따르는 분포를 자유도가 $k_1, k_2$인 F분포라고 정의한다. > 2개의 카이제곱분포의 비율이 만들어내는 분포
    
    $$ f(x) = \frac{r\left(\frac{k_1+k_2}{2}\right)}{r\left(\frac{k_1}{2}\right)r\left(\frac{k_2}{2}\right)}\left(\frac{k_1}{k_2}\right)^{\frac{k_1}{2}}x^{\frac{k_1}{2}-1}\left(1 + \frac{k_1}{k_2}x\right)^{-\frac{1}{2}(k_1 + k_2)}, 0 < x < \infty$$
    
    - 이 경우 $X \sim F[k_1, k_2]$라고 한다.

- F분포의 특성치

    - $X \sim F[k_1, k_2]인 경우, $E[X] = \frac{k_2}{k_2-2}, V[X] = \frac{2k\frac{2}{2}(k_1+k_2-2)}{k_1(k_2-2)^2(k_2-4)}$를 만족한다.

- F분포 확률밀도함수 개형

    - 카이제곱 분포처럼 오른쪽으로 치우친 비대칭 구조이다.

- F확률변수의 $(1 - \alpha)$분위수: $F_{\alpha, k_1, k_2}$

    - $X \sim F[k_1, k_2]$일 때, $P[X > c] = \alpha$를 만족하는 X의 $(1 - \alpha)$분위수 c를 $F_{\alpha, k_1, k_2}$으로 표기한다.

# 탐색적 데이터 분석

## 그래프에 의한 기술통계

### 데이터 시각화 개요

- 그래프를 이용한 자료의 정리: 자료의 유형에 맞는 그래프를 이용하여 한눈에 알아볼 수 있게 자료를 시각화할 수 있음

- 질적 자료인 경우 변수 1개일 때 바차트, 파이차트를 확인할 수 있고, 변수 2개인 경우 히트맵, 스택드컬럼차트를 확인할 수 있다.

- 양적 자료인 경우 변수 1개일 때 히스토그램, 박스플롯, 라인차트, QQ플롯을 확인할 수 있고, 변수 2개일 때 산점도를 확인할 수 있다.

## 수치적 기술통계

### 위치, 변이, 모양통계량

#### 1. 중심 위치 척도

- 평균

    - 표본 자료 $x_1, ..., x_n$이 주어졌을 때, 전체 데이터의 합을 개수로 나눈 것을 표본평균이라고 한다.
    
    - 이상치에 민감하다는 단점이 있다.

- 중위수, 중앙값(Median)

    - 표본 자료 $x_1, ..., x_n$을 오름찻훈으로 정렬하였을 때, 중간에 위치한 값.
    
    - n이 홀수인 경우 $\frac{n+1}{2}$번째 자료, n이 짝수인 경우 $\frac{n}{2}$번째와 $\frac{n}{2}+1$번째 자료의 평균이다.
    
    - 평균과 다르게 이상치에 민감하지 않다.

- 최빈값(Mode)

    - 가장 빈도가 높은 값 또는 구간을 말한다.

- 분포의 치우침 유형별 척도들 간의 관계

    - 오른쪽 꼬리가 긴 경우(왼쪽으로 치우친 경우): 최빈값 < 중앙값 < 평균 순서
    
    - 가운데를 중심으로 대칭형(종 모양): 최빈값 = 중앙값 = 평균
    
    - 왼쪽 꼬리가 긴 경우(오른쪽으로 치우친 경우): 평균 < 중앙값 < 최빈값 순서

#### 2. 상대적 위치 척도

- 사분위수(Quartile): Q1, Q2, Q3

    - Q1: 25%에 해당하는 수. 1사분위수
    
    - Q2: 중앙값. 2사분위수
    
    - Q3: 75%에 해당하는 수. 3사분위수

#### 3. 변동성 척도

- 범위(Range)

    - 표본 자료 $x_1, ..., x_n$이 주어졌을 때, $max(x_i) = min(x_i)$를 범위라고 한다.

- 사분위간 범위(IQR, Inter Quartile Range)

    - Q3 - Q1

- 표본 분산(Sample Variance)

    - 표본 자료 $x_1, ..., x_n$이 주어졌을 때, 
    
    $$ s^2 = \frac{\sum_{i=1}^{n} (x_i - \overline{x})^2}{n - 1} $$

- 표본 표준편차(Sample Standard Deviation)

    - $ s = \sqrt{s^2}$

- 변동계수(Coefficient of Variation)

    - $ cv = s/\overline{x} $
    
    - 자료의 단위가 다르거나 스케일의 차이가 큰 경우 활용한다.

#### 4. 형태 척도

- 분포의 형태에 관한 척도

    - 왜도: 분포의 비대청 정도를 나타내는 척도. 음의 값인 경우 오른쪽으로 치우쳐진 분포이고, 양의 값인 경우 왼쪽으로 치우쳐진 분포이다.
    
    - 첨도: 분포의 중심에서 뽀족함 정도를 나타내는 척도. 정규분포인 경우는 첨도가 0이며, 양수가 나오는 경우 정규분포보다 더 뽀족한 형태이고, 음수인 경우는 정규분포보다 더 평평한 형태이다.

### 연관성

#### 1. 선형적 연관성

- 두 숫자형 변수의 선형적 연관성

    - 선형적 연관성의 방향: 양의 선형관계의 경우 오른쪽 위로 향하는 형태, 음의 선형관계의 경우 오른쪽 아래로 향하는 형태이다.
    
    - 선형적 연관성의 강도: 모여있는 정도에 따라 약한지, 강한지 파악

#### 2. 선형적 연관성 척도

- 표본 공분산(Sample Covariance)

    - n쌍의 표본 자료가 주어졌을 때, 
    
    %% s_{xy} = \frac{\sum_{i=1}^{n}(x_i - \overline{x}(y_i - \overline{y}))}{n-1} $$
    
    - 선형관계의 방향
    
        - $s_{xy} > 0$: 양의 선형 관계, 비례 관계
        
        - $s_{xy} < 0$: 음의 선형 관계, 반비례 관계
    
    - 선형관계의 강도
    
        $ -S_xS_y <= S_{xy} <= s_xs_y $
    
    - 표본 공분산은 x와 y의 측정 단위에 의존하는 지표이다.<br>
    $ x^{a} = ax + b$이고, $y^{a} = cy + d$인 경우, $s_{x^{a}y^{a}} = ac * S_{xy}$이다.

- 표본 상관계수(Sample Correlation, 피어슨 상관계수)

    - $r_{xy} = \frac{S_{xy}}{S_xS_y}$
    
    - $ -1 <= r_{xy} <= 1 $
    
    - 선형관계의 방향
    
        - $r_{xy} > 0$: 양의 선형관계
        
        - $r_{xy} < 0$: 음의 선형관계
    
    - 선형관계의 강도:
    
        - $|r_{xy}| \approx 0$: 강도가 약함
        
        - $|r_{xy}| \approx 1$: 강도가 강함
    
    - 표본 상관계수는 x와 y의 측정 단위에 의존하지 않음.
    
        - $ x^{a} = ax + b, y^{a} = cy + d $이고, ac > 0인 경우, $ r_{x^{a}y^{a}} = r_{xy} $

- 순위를 이용한 상관계수

    - 서열 척도이거나 정규분포를 심하게 벗어나는 두 숫자형 변수의 연관성을 파악할 때 사용
    
    - 스피어만 상관계수(Spearman's correlation coefficient)
    
        - 원 자료값의 순위를 구한 뒤, 순위에 대해 피어슨의 상관계수를 구한 것
        
        - -1에서 1사이의 값을 가지며, 절대값이 클수록 강한 상관관계를 나타낸다.
    
    - 켄달 상관계수(Kendall rank correlation ceofficeient, 켄달의 타우)
    
        - 두 변수 순위의 일치 정도를 측정
        
        - 한 변수의 순위가 증가할 때 다른 변수의 순위도 함께 증가하는 경우가 그렇지 않은 경우에 비해 얼마나 큰지를 측정하는 방식
        
# 추정과 검정

## 통계적 추론

### 통계적 추론 개요

- 모수와 통계량 각각의 기호

    - 평균: $\mu$(모평균), $\overline{X}$(표본평균)
    
    - 분산: $\sigma = V[X]$(모분산), $S^2$(표본분산)
    
    - 표준편차: $\sigma$(모표준편차), $S$(표본표준편차)

- 모집단의 분포와 확률표본

    - 모집단의 변수 X의 확률분포함수를 f(x)로 가정.
    
    - 모집단 f(x)로부터의 확률표본(iid 표본) $X_1, X_2, ..., X_n$은 다음의 두가지 성질을 만족하는 모집단 f(x)로부터의 표본을 뜻한다.
    
        - 각 표본은 서로 독립이다.
        
        - 표본은 모두 동일하게 f(x)의 분포를 따른다.

- 통계량과 표본분포

    - 통계량: 확률표본 $X_1, X_2, ..., X_n$의 함수에서 평균, 분산, 표준편차 등
    
    - 표본분포: 통계량의 확률분포

### 표본추출법

- 표본추출 방법

    - 모집단을 대표할 수 있도록 표본을 추출하는 것이 가장 중요하다.
    
    - 표본추출방법에 따라 분석 결과 및 해석에 큰 차이가 발생할 수 있다.
    
    - 대표적인 4가지 표본 추출방법: 단순임의 추출법(simple random sampling), 계통추출법(systematic sampling), 집락추출법(cluster sampling), 층화추출법(stratified sampling)

- 단순임의 추출법

    - 모집단을 구성하는 모든 원소에 대해 표본으로 추출될 확률을 동일하게 해주는 표본 추출법
    
    - 난수표에서 난수를 발생시켜 표본을 추출
    
        - N개의 원소로 구성된 모집단에서 $n(<=N)$개의 표본을 추출할 때 각 원소에 1, 2, ..., N까지의 번호를 부여하고 여기서 n개의 번호를 임의로 선택해 그 번호에 해당하는 원소로 표본을 추출하는 방법

- 계통 추출법

    - N개의 원소로 구성된 모집단에서 n개의 표본을 추출할 때, 각 원소에 1, 2, ..., N까지의 번호를 부여하고 이를 순서대로 나열한다.
    
    - K개씩 n개의 구간으로 나눈 뒤 첫번째 구간에서 K개의 원소 중 하나를 임의로 선택하고, 그 이후 K개씩 띄어서 표본을 추출

- 집락 추출법

    - 모집단이 몇 개의 집락(cluster)이 결합된 형태로 구성되어 있고, 각 집락에서 원소들에게 일련번호를 부여할 수 있는 경우에 이용
    
    - 일부 집락을 랜덤하게 선택한 뒤, 선택된 각 집락에서 표본을 임의로 선택한다.

- 층화 추출법

    - 상당히 이질적인 원소들로 구성된 모집단에서 각 계층을 고루 대표할 수 있도록 표본을 추출. 이질적인 모집단의 원소들을 서로 유사한 것끼리 몇 개의 층(stratum)으로 나눈 후, 각 층에서 표본을 랜덤하게 추출한다.

## 추정

### 통계적 추론 - 추정 개요

- 추정: 모수를 맞추는 것

- 추정량: 모수 $\theta$의 추정에 사용되는 통계량을 $\theta$의 추정량($\widehat{\theta}$으로 표기)이라고 한다. 관찰된 표본자료로 추정량의 값을 계산한 것을 추정치라고 한다.

- 점추정과 구간추정

    - 점추정(point estimation): 하나의 모수를 한 개의 값으로 추정
    
    - 구간 추정(interval estimation): 모수가 포함되리라 기대되는 구간으로 모수를 추정

- 신뢰구간

    - 모수 $\theta$에 대한 신뢰구간 도출을 위해서 알아야 하는 것은 추정량 $\widehat{\theta}$, 추정량 $\widehat{\theta}$의 표본분포(일반적으로 모수 $\theta$에 의존)가 있다.
    
    - 신뢰구간의 정의: $\theta$의 추정량 $\widehat{\theta}$을 적절하게 변형한 L과 U이 있어 다음을 만족하는 경우, $$ P[L <= \theta <= U] = 1 - \alpha, 0 <= \alpha <- 1$$ 구간 (L, U)을 $\theta$의 100(1-$\alpha$)% 신뢰구간이라고 한다. 여기서 100(1-$\alpha$)%를 신뢰수준이라고 한다.
    
    - 신뢰구간의 해석
    
        - 신뢰수준은 동일한 구간 추정법을 반복적으로 사용할 때 얻어지는 신뢰구간들이 참값 $\theta$를 품을 확률을 의미한다.
        
        - $\mu$에 관한 추정에서 신뢰수준 90%의 의미는 n개의 표본을 추출하는 과정을 반복하여 매번 이 구간 추정식으로 신뢰구간을 계산한다면, 이들 중 90%에 해당하는 신뢰구간들이 참값 $\mu$를 포함하는 것을 의미
        
        - n개의 표본으로 구한 하나의 신뢰구간에 모수가 포함되었는지의 여부는 알 수 없다.

## 가설검정

### 기본 개념

- 통계적 가설검정이란 표본으로부터 주어지는 정보를 이용하여 모수에 대한 예상, 주장 또는 추측 등의 옳고 그름을 판정하는 과정이다.

- 가설

    - 통계적 가설은 모수에 대한 예상, 주장 또는 추측을 표현한 것이다.
    
    - 귀무가설($H_0$): 지금까지 사실로 알려져 있는 가설
    
    - 대립가설($H_1$): 표본자료로부터의 강력한 증거에 의해 입증하고자 하는 가설
    
    - 가설검정은 표본의 정보가 귀무가설 $H_0$에 대한 충분한 반증이 되는가를 보는 것
    
    - 결론도 "귀무가설 $H_0$을 기각" 또는 "귀무가설 $H_0$을 기각하지 못함" 중 하나로 표현한다.

- 가설 유형: 관심 모수가 $\mu$이고, 검정하고자 하는 모수의 경계값이 $\mu_0$라고 할 때, 단측 검정과 양측 검정이 있다.

    - 단측 검정
    
        - 왼 꼬리 검정: $H_0: \mu = \mu_0, H_1: \mu < \mu_0$
        
        - 오른 꼬리 검정: $H_0: \mu = mu_0, H_1: \mu > \mu_0$
    
    - 양측 검정: $H_0: \mu = \mu_0, H_1: \mu \neq \mu_0$

- 검정통계량

    - 귀무가설 $H_0$과 대립가설 $H_1$ 중 어느 하나를 택하는 기준을 결정하는 통계량
    
    - 모집단이 모분산이 알려진 정규분포인 경우, 모평균 $\mu$에 관한 검정의 경우,
    
        - 검정통계량: 표본평균 $\overline{X}$의 함수인 $ Z = \frac{\overline{X} - \mu}{\sigma/\sqrt{n}}$
        
        - 검정통계량 표본분포: $Z = \frac{\overline{X} - \mu}{\sigma/\sqrt{n}} \sim N[0, 1]$
        
        - 귀무가설 $H_0: \mu = \mu_0$이 사실일 때, $ Z = \frac{\overline{X} - \mu}{\sigma/\sqrt{n}} \sim N[0, 1]$

- 귀무가설 $H_0$의 기각여부를 판단하기 위한 두 가지 접근 방식

    - 기각역(귀무가설 $H_0$을 기각하게 하는 검정통계량 관측치의 영역)에 의한 검정
    
    - 유의확률에 의한 검정
    
    - 두 방식은 동일한 결론을 가진다.

- 유의확률에 의한 검정 방법

    - 유의확률(p-value): 귀무가설 $H_0$가 사실인 경우의 검정통계량 분포에서, 대립가설 $H_1$의 방향으로, 검정통계량의 관찰값보다 더 극단적인 값이 나올 확률로 계산
    
    - 모분산 $\sigma^2$이 알려진 정규 모집단에서 모평균 $\mu$에 관한 검정의 경우 귀무가설 $H_0$이 사실일 때, 검정통계량 Z의 분포에서 $z_0$(= 표본자료로부터 계산된 검정통계량의 값)을 이용하여 계산
    
    - 유의확률의 해석: 유의확률은 값이 작을수록 해당하는 표본은 귀무가설 $H_0$가 사실인 경우의 표본으로 보기 어려우며, 대립가설 $H_1$을 더욱 지지하는 것으로 해석할 수 있다. 즉, 유의확률이 일정수준보다 작으면 귀무가설 $H_0$를 기각한다.

- 가설검정에서의 의사결정에서 발생할 수 있는 오류

    - 제 1종 오류: 실제로는 귀무가설 $H_0$가 사실인데, 귀무가설 $H_0$을 기각하게 되는 오류이며, 제 1종 오류의 확률을 $\alpha$로 표기한다.
    
    - 제 2종 오류: 실제로는 대립가설 $H_1$이 사실인데, 귀무가설 $H_0$을 기각하지 못하게 되는 오류이며, 제 2종 오류의 확률을 $\beta$로 표기한다.

- 유의수준 100 $\alpha$ %의 검정법

    - 검정의 오류: 두 종류의 오류는 서로 상충관계로 동시에 줄이기 어렵다. 그렇기에 더 위험한 제1종 오류가 일정 수준을 넘지 못하도록 제한하면서 제2종 오류를 최소로 하는 검정법을 찾고자 한다.
    
    - 유의수준: 제1종 오류를 범할 확률의 최대 허용 한계
    
    - 자료로부터 계산된 유의확률(p-value)이 주어진 유의수준 $\alpha$보다 작은 경우에 귀무가설 $H_0$를 기각한다. 즉, 유의확률(p-value) <= 유의수준($\alpha$)면, 귀무가설 $H_0$를 기각

### 모평균에 관한 가설검정(one sample t-test)

- 모평균에 관한 추론: $\mu$에 관한 추론으로, 추정량 $\overline{X}$의 표본분포가 필요하다.

- 모집단이 정규분포인 경우 표본평균의 표본분포

    - 모집단이 정규분포이고, 모분산이 $\sigma^2$이 알려진 경우, $$ Z = \frac{\overline{X} - \mu}{\sigma / \sqrt{n}} \sim N[0, 1] $$
    
    - 모집단이 정규분포이고, 모분산 $\sigma^2$이 알려지지 않은 경우, $$ T = \frac{\overline{X} - \mu}{\sigma / \sqrt{n}} \sim t[n - 1] $$
    단, S는 표본표준편차로 $S = \sqrt{\frac{\sum_{i=1}^{n}(X_i-X)^2}{n-1}}$로 계산된다.

- 모평균에 관한 가설검정(one sample t-test)

    - 가설 유형: 관심 모수가 $\mu$이고 검정하고자 하는 모수의 경계값이 $\mu_0$라고 할 때, 
    
        - 단측 검정 중 왼 꼬리 검정: $H_0: \mu = \mu_0, H_1: \mu < \mu_0$
        
        - 단측 검정 중 오른 꼬리 검정: $H_0: \mu = \mu_0, H_1: \mu > \mu_0$
        
        - 양측 검정: $H_0: \mu = \mu_0, H_1: \mu \neq \mu_0$
    
    - 검정통계량
    
        - 모집단이 정규분포이고 모분산 $\sigma^2$이 알려지지 않은 경우, $$ T = \frac{\overline{X} - \mu}{S / \sqrt{n}} \sim t[n - 1]$$
        
        - 검정통계량 T는 귀무가설 $H_0: \mu = \mu_0$이 사실일 때, $T = \frac{\overline{X} - \mu_0}{S / \sqrt{n}} \sim t[n - 1]$를 만족한다.
    
    - 유의수준(p-value)의 계산 및 유의수준 100($\alpha$)% 검정법
    
        - 귀무가설 $H_0$가 사실일 때, 검정통계량 T의 분포 $T = \frac{\overline{X} - \mu_0}{S / \sqrt{n}} \sim t(n - 1)$에서, $t_0$(= 표본 자료로부터 게산된 검정통계량의 값)보다 대립가설 방향으로 더 극단적인 값이 나올 확률
        
        - 자료로부터 계산된 유의확률(p-value)이 주어진 유의수준 $\alpha$보다 작은 경우에 귀무가설 $H_0$를 기각함. p-value <= $\alpha$면, %H_0$를 기각.

### 모평균 비교에 관한 가설검정(Independent two sample t-test)

- 두 모집단의 모평균 차이($\mu_1 - \mu_2)

    - 독립인 두 표본이고, 등분산을 가정하며, 정규 모집단이라는 것을 전제로 한다.
    
    - $\theta = \mu_1 - \mu_2$에 관한 추론: 추정량($\widehat{\theta} = \overline{X_1} - \overline{X_2}$)의 표본분포를 이용한다.

- 추정량($\widehat{\theta} = \overline{X_1} - \overline{X_2}$)의 표본분포

    - 두 모집단이 정규분포이고, 모분산 $\theta^2$이 알려지지 않았지만 동일한 것으로 가정하며, 두 표본은 독립인 상태
    
    - $\widehat{X_1} - \widehat{X_2} \sim N[\mu_1 - \mu_2, \sigma^2(1/n_1 + 1/n_2)]$
    
    - $Z = \frac{\widehat{X_1} - \widehat{X_2} - (\mu_1 - \mu_2)}{\sqrt{\sigma^2(1/n_1 + 1/n_2)}} \sim N[0, 1]$

    - $ T = \frac{\widehat{X_1} - \widehat{X_2} - (\mu_1 - \mu_2)}{\sqrt{S_p^2(1/n_1 + 1/n_2)}} \sim t[n_1 + n_2 - 2]$ > 여기서 $S_p^2$는 합동분산추정량이다. $$ S_p^2 = \frac{(n_1 - 1)S_1^2 + (n_2 - 1)S_2^2}{n_1 + n_2 - 2}$$

- 모평균 비교에 관한 독립 이표본 t-검정

    - 가설 유형: 관심 모수가 $\mu_1 - \mu_2$이고 검정하고자 하는 모수의 경계값은 0이 된다.
    
        - 단측(왼 꼬리) 검정: $ H_0: \mu_1 - \mu_2 = 0, H_1: \mu_1 - \mu_2 < 0$
        
        - 단측(오른 꼬리) 검정: $ H_0: \mu_1 - \mu_2 = 0, H_1: \mu_1 - \mu_2 > 0$
        
        - 단측(왼 꼬리) 검정: $ H_0: \mu_1 - \mu_2 = 0, H_1: \mu_1 - \mu_2 \neq 0$
    
    - 검정통계량
    
        - 검정통계량 T는 귀무가설 $H_0: \mu_1 - \mu_2 = 0$이 사실일 때, $T = \frac{\overline{X_1} - \overline{X_2}}{\sqrt{S_p^2(1/n_1 + 1/n_2)}} \sim t[n_1 + n_2 - 2]$를 만족한다.
    
    - 유의확률(p-value)의 계산
    
        - 귀무가설 $H_0$가 사실일 때, 검정통계량 T의 분포 $T = \frac{\overline{X_1} - \overline{X_2}}{\sqrt{S_p^2(1/n_1 + 1/n_2)}} \sim t[n_1 + n_2 - 2]$에서, $t_0$(= 표본자료로부터 계산된 검정통계량의 값)보다 대립가설 방향으로 더 극단적인 값이 나올 확률
    
    - 유의수준 100$\alpha$%의 검정법
    
        - 자료로부터 계산된 유의확률(p-value)이 주어진 유의수준 $\alpha$보다 작은 경우에 귀무가설 $H_0$을 기각. 즉, p-value <= $\alpha$면, $H_0$를 기각한다.

### 모평균 비교에 관한 가설검정(paired t-test)

- 대응표본: 두 모집단으로부터 추출된 서로 짝을 이루는 표본. 예를 들어 체중, 나이 등이 유사한 2명씩 짝을 지어 한 명은 A약, 다른 한 명은 B약을 투여한 뒤 혈압을 측정하여 비교하는 것이 있다.

- 대응표본에 대한 모평균 차이($\mu_d$): $\mu_d$에 관한 추론의 경우 추정량($\overhat{d}$)의 표본분포를 이용한다.

- 대응표본에서 모평균 비교에 관한 쌍체 t-검정

    - 가설 유형: 관심 모수가 $\mu_d$이고 검정하고자 하는 모수의 경계값은 0이 된다.
    
        - 단측(왼 꼬리) 검정: $ H_0: \mu_d = 0, H_1: \mu_d < 0$
        
        - 단측(오른 꼬리) 검정: $ H_0: \mu_d = 0, H_1: \mu_d > 0$
        
        - 단측(왼 꼬리) 검정: $ H_0: \mu_d = 0, H_1: \mu_d \neq 0$
    
    - 검정통계량과 표본분포
    
        - 대응표본간 차이 $d_1, ..., d_n$이 분산이 알려지지 않은 정규분포를 따른다고 가정한다.
        
        - 검정통계량 T는 귀무가설 $H_0: \mu_d = 0$이 사실일 때, $T = \frac{\d}{S_d/\sqrt{n}} \sim t[n - 1]$를 만족한다. 단, $\overline{d} = \frac{\sum_{i=1}^n}{n}, S_d = \sqrt{\frac{\sum_{i=1}^n(d_i-\overline{d})^2}{n-1}}$이다.
    
    - 유의확률(p-value)의 계산
    
        - 귀무가설 $H_0$가 사실일 때, 검정통계량 T의 분포 $T = \frac{\d}{S_d/\sqrt{n}} \sim t[n - 1]$에서, $t_0$(= 표본자료로부터 계산된 검정통계량의 값)보다 대립가설 방향으로 더 극단적인 값이 나올 확률
    
    - 유의수준 100$\alpha$%의 검정법
    
        - 자료로부터 계산된 유의확률(p-value)이 주어진 유의수준 $\alpha$보다 작은 경우에 귀무가설 $H_0$을 기각. 즉, p-value <= $\alpha$면, $H_0$를 기각한다.

### 모평균 비교에 관한 가설검정(One way ANOVA 및 사후검정)

- 분산분석(Analysis of Variance, ANOVA) 개요

    - 2개 이상의 모집단의 모평균의 차이를 검정
    
    - 평균 차이를 파악하기 위해 변동성을 이용: 그룹별 평균이 다르면 그룹별 평균의 변동성이 크다는 사실을 이용한다.
    
    - 용어
    
        - 요인(factor): 모집단(그룹)의 구분기준
        
        - 처리(treatment(levels)): 요인을 구성하는 각 모집단(그룹)

- 일원분산분석(One Way ANOVA)

    - 일원배치 자료의 구조: 1개의 요인에 대해 k개의 처리(그룹)로 분류되어 있는 자료.
    
    - 일원배치 분산분석 예시: 3가지 타입의 자동차 헤드라이트의 디자인을 고려하는데, 라이트의 효과를 비교하기 위해 각 디자인을 적용한 차량별로 시속 60km의 속도에서 장애물을 인지하는 거리(m)를 5번씩 반복해 측정했다. 이 자료를 토대로 라이트 종류에 따라 인지거리에 차이가 있는지를 유의수준 n%로 검정하고자 한다.
    
    - 일원배치 분산분석 가정
    
        - 정규분포, 등분산, 독립이라고 가정
        
        - 1개의 요인에 대해 k개의 처리(그룹)로 분류되어 있는 자료
        
        - k개의 모평균 $\mu_1, \mu_2, ..., \mu_k$이 모두 동일한지 아닌지를 검정하고자 한다.
    
    - 제곱합의 분해
    
        - $\sum_{i=1}^k\sum_{j=1}^n(y_{ij} - \overline{y})^2 = \sum_{i=1}^k\sum_{j=1}^n(y_{ij} - \overline{y_i})^2 + \sum_{i=1}^k\sum_{j=1}^n(\overline{y_i} - \overline{y})^2$
        <br>총변동성(SST) = 오차에 의한 변동(SSE, 그룹 내 변동) + 요인에 의한 변동(SSTR, 그룹 간 변동)
        
        - 총제곱합(SST): 그룹에 무관한 전체 자료의 변동을 측정
        
        - 오차제곱합(SSE): 각 그룹 내에서의 변동을 측정
        
        - 요인제곱합(SSTR): 각 그룹별 평균의 변동을 측정
        
            - SSTR의 크기로 집단별 모평균 차이를 파악할 수 있다.
    
    - 그룹내 변동과 그룹간 변동
    
        - 그룹내 변동과 그룹간 변동이 모두 동일한 경우의 그룹간 변동은 그룹내 변동에 의존한다.
        
        - 그룹간 모평균 차이를 파악하기 위해서는 그룹내 변동에 비해 그룹간 변동의 상대적인 크기를 측정해야 한다.

- One way ANOVA 검정 절차

    - 가설
    
        - 귀무가설 $H_0$: 집단간 평균의 차이가 없음
        
        - 대립가설 $H_1$: 집단간 평균의 차이가 존재함(해당 요인의 처리 효과가 있음)
    
    - 검정 통계량
    
        - 평균 제곱: 제곱합들을 각각의 자유도로 나눈 값. $$ MSTR = SSTR / (k - 1), MSE = SSE / (nk - k)$$
        
        - 만일 그룹별 모평균이 같다면 $MSTR \approx MSE$, 만일 그룹별 모평균이 모두 같지 않다면 $MSTR \gg MSE$
        
        - 귀무가설($H_0$)이 사실인 경우, $$X = \frac{MSTR}{MSE} \sim F[k - 1, nk - k] $$
    
    - 유의확률(p-value)의 계산
    
        - 귀무가설 $H_0$가 사실일 때, 검정통계량 X의 분포 $ X = \frac{MSTR}{MSE} \sim F[k - 1, nk - k]$에서 $x_0$(= 표본 자료로부터 계산된 검정통계량의 값)보다 더 큰 값이 나올 확률
    
    - 유의수준 100$\alpha$%의 검정법
    
        - 자료로부터 계산된 유의확률(p-value)이 주어진 유의수준 $\alpha$보다 작은 경우에 귀무가설 $H_0$을 기각. 즉, p-value <= $\alpha$면, $H_0$를 기각한다.

- 사후검정

    - 분산분석은 모든 집단의 모평균이 동일한지, 하나 이상의 집단에서 모평균의 차이가 나는지에 대해서만 파악한다. 따라서 집단별 차이가 존재하는 것으로 결론이 나더라도 어느 집단에서 차이가 발생하는지 알 수 없다.
    
    - 사후 검정을 통해 어떤 집단에서 차이가 발생하는지를 확인할 수 있다.
    
    - Fisher의 LSD 방식, Tukey의 W 방식, Duncan의 Multiple Range Test, Scheffe의 S 방법, Dunnett의 방식 등이 있다.

### 모평균 비교에 관한 가설검정(Two way ANOVA)

- 요인 설계 및 이원배치 분산분석 개요

    - 요인설계(Factorial Design): 반응변수에 영향을 미치는 요인의 수가 여러 개인 경우, 요인설계를 적용해야 한다.
    
    - 이원배치 분산분석(Two way ANOVA)
    
        - 가장 단순한 요인설계 분석법으로, 요인이 2개, 각 요인 별 처리수준이 2개 이상인 경우
        
            - 주 효과: 각 요인별 처리수준 간의 모평균 차이에 관한 분석
            
            - 상호작용 효과: 두 요인 간 상호작용에 관한 분석
        
        - 완전 확률화 과정: 실험대상은 각 요인의 처리수준별 조합(셀)에 무작위로 배치된다.
        
        - 가정: 정규분포, 등분산, 독립

- 이원배치 분산분석

    - 제곱합의 분해
    
        - SST(총변동성) = SSA(요인A 처리변동) + SSB(요인B 처리변동) + SSAB(요인 A, B의 상호작용변동) + SSE(오차변동)
        
        - (abn - 1) = (a-1) + (b-1) + (a-1)(b-1) + ab(n-1)
    
    - 가설검정 절차
    
        - 가설
        
            1. 요인 A의 처리효과에 관한 가설
            
                - $H_0$: 요인 A의 처리 그룹 간 평균의 차이가 없음. 처리효과 없음
                
                - $H_1$: 요인 A의 처리 그룹 간 평균의 차이가 존재. 처리효과 있음
            
            2. 요인 B의 처리효과에 관한 가설
            
                - $H_0$: 요인 B의 처리 그룹 간 평균의 차이가 없음. 처리효과 없음
                
                - $H_1$: 요인 B의 처리 그룹 간 평균의 차이가 존재. 처리효과 있음
            
            3. 요인 A와 요인 B의 상호작용 효과에 관한 가설
            
                - $H_0$: 요인 A와 요인 B의 상호작용 효과가 없음
                
                - $H_1$: 요인 A와 요인 B의 상호작용 효과가 존재
        
        - 검정통계량의 표본분포
        
            1. 요인 A의 처리효과: 요인 A에 관한 $H_0$이 사실인 경우, 다음 식을 따른다. $$ F_1 = \frac{MSA}{MSE} = \frac{SSA/(a-1)}{SSE/(ab(n-1))} \sim F[a-1, (ab(n-1))]$$
            
            2. 요인 B의 처리효과: 요인 B에 관한 $H_0$이 사실인 경우, 다음 식을 따른다. $$ F_2 = \frac{MSB}{MSE} = \frac{SSB/(ba-1)}{SSE/(ab(n-1))} \sim F[b-1, (ab(n-1))]$$
            
            3. 요인 A와 요인 B의 상호작용효과: 요인 A와 요인 B의 상호작용에 관한 $H_0$이 사실인 경우, 다음 식을 따른다. $$ F_3 = \frac{MSAB}{MSE} = \frac{SSAB/(a-1)(b-1)}{SSE/(ab(n-1))} \sim F[(a-1)(b-1), (ab(n-1))]$$
        
        - 유의확률(p-value)의 계산
        
            - 귀무가설 $G_0$가 사실일 때 검정통계량의 분포에서 $f_0$(= 표본자료로부터 계산된 검정통계량의 값)보다 더 큰 값이 나올 확률
        
        - 유의수준 100$\alpha$%의 검정법
    
            - 자료로부터 계산된 유의확률(p-value)이 주어진 유의수준 $\alpha$보다 작은 경우에 귀무가설 $H_0$을 기각. 즉, p-value <= $\alpha$면, $H_0$를 기각한다.

### 모분산 비교에 관한 가설검정(F-test of equality of variances)

- 두 모집단의 모분산 비교

    - 독립인 두 표본에서 $\theta = \sigma_1^2 / \sigma_2^2$에 관한 추론 > 추정량($\widehat{\theta} = S_1^2 / S_2^2$)의 표본분포를 이용한다. $$ X = \left(\frac{\sigma_2^2}{\sigma_1^2}\right)\left(\frac{S_1^2}{S_2^2}\right) \sim F[n_1 - 1, n_2 - 1] $$

- 모분산 비교에 관한 가설검정

    - 가설 유형: 관심 모수가 $\sigma_1^2/\sigma_2^2$이고 검정하고자 하는 모수의 경계값은 1
    
        - 단측(왼 꼬리) 검정: $ H_0: \sigma_1^2/\sigma_2^2 = 1, H_1: \sigma_1^2/\sigma_2^2 < 1$
        
        - 단측(오른 꼬리) 검정: $ H_0: \sigma_1^2/\sigma_2^2 = 1, H_1: \sigma_1^2/\sigma_2^2 > 1$
        
        - 단측(왼 꼬리) 검정: $ H_0: \sigma_1^2/\sigma_2^2 = 1, H_1: \sigma_1^2/\sigma_2^2 \neq 1$
    
    - 검정통계량
    
        - 두 모집단이 모두 정규분포인 경우 다음의 검정통계량 X는 귀무가설 $H_1: \sigma_1^2/\sigma_2^2 = 1$이 사실일 때, 다음 식을 만족한다. $$ X = 1\left(\frac{S_1^2}{S_2^2}\right) \sim F[n_1 - 1, n_2 - 1] $$
    
    - 유의확률(p-value)의 계산
    
        - 귀무가설 $H_0$가 사실일 때, 검정통계량 X의 분포 $ X = 1\left(\frac{S_1^2}{S_2^2}\right) \sim F[n_1 - 1, n_2 - 1] $에서, $x_0$(= 표본 자료로부터 계산된 검정통계량의 값)보다 대립가설 방향으로 더 극단적인 값이 나올 확률
    
    - 유의수준 100$\alpha$%의 검정법
    
        - 자료로부터 계산된 유의확률(p-value)이 주어진 유의수준 $\alpha$보다 작은 경우에 귀무가설 $H_0$을 기각. 즉, p-value <= $\alpha$면, $H_0$를 기각한다.

- Bartlett 검정과 Levene 검정

    - k(>= 2)개의 모집단에 대한 등분산 검정
    
    - 가설
    
        - $H_0: \sigma_1^2 = \dots = \sigma_k^2$
        
        - $H_1$: 적어도 하나 이상의 $\sigma_i^2$은 나머지와 다르다.
        
    - Bartlett 검정
    
        - 모집단이 모두 정규분포를 따르는 경우 적용
        
        - k = 2인 경우 등분산 F검정, k > 3인 경우는 Bartelett의 검정
    
    - Levene 검정: 모집단에 대한 정규성 가정이 필요하지 않다.

### 범주형 변수 간의 독립성 검정(Chi-squared test)

- 카이제곱 검정 개요

    - 범주형인 자료를 분석하는데 사용
    
    - 범주형 자료 분석은 크게 적합도 검정(goodness of fit test), 독립성 검정(test of independence)으로 나누어진다.
    
        - 적합도 검정: 하나의 범주형 변수에 대해 각 범주별 확률에 관한 검정
        
        - 독립성 검정: 서로 다른 두 범주형 변수 간에 연관성이 있는지를 검정

- 카이제곱 독립성(Independence) 검정

    - 2개의 범주형 변수를 요약하는 $\gamma X c$의 2차원 분할표
    
        - 두 변수가 독립인 경우에 예상되는 각 셀별 기대빈도 도출
        
        - 각 셀별 빈도와 기대빈도의 차이 크기를 이용해 검정
        
        - 차이가 작으면 두 변수는 독립 / 차이가 크면 두 변수는 독립이 아님

- 카이제곱 독립성 검정 절차

    - 가설
    
        - $H_0$: 두 범주형 변수는 서로 독립이다(관계가 없다).
        
        - $H_1$: 두 범주형 변수는 독립적인 관계가 아니다(관계가 있다).
    
    - 검정통계량과 표본분포
    
        - 귀무가설($H_0$)이 사실인 경우, 표본의 수가 충분히 큰 경우(모든 기대빈도 5 이상)일 때 다음을 만족한다. $$ X = \sum_{j=1}^{c}\sum_{i=1}^{\gamma}\frac{(X_{ij} - n\widehat{p}_{ij})^2}{n\widehat{p}_{ij}} \sim X^2[(\gamma - 1)(c - 1)] $$
    
    - 유의확률(p-value)의 계산
    
        - 귀무가설($H_0$)이 사실일 때, 검정통계량 X의 표본분포 $ X = \sum_{j=1}^{c}\sum_{i=1}^{\gamma}\frac{(X_{ij} - n\widehat{p}_{ij})^2}{n\widehat{p}_{ij}} \sim X^2[(\gamma - 1)(c - 1)] $에서, $x_0$(= 표본 자료로부터 계산된 검정통계량의 값)보다 더 큰 값이 나올 확률
    
    - 유의수준 100$\alpha$%의 검정법
    
        - 자료로부터 계산된 유의확률(p-value)이 주어진 유의수준 $\alpha$보다 작은 경우에 귀무가설 $H_0$을 기각. 즉, p-value <= $\alpha$면, $H_0$를 기각한다.

# 시계열 분석

## 시계열 데이터의 특징, 시계열 분해법, 자기상관

### 시계열 분석

- 시계열 자료의 개념: 시간의 흐름에 따라 순차적으로 관찰된 자료들의 집합

- 시계열 자료의 예시: 월별 소비자 물가지수, 일별 주가지수 / 일일 강수량, 일일 기온, 연간 지진의 발생 수 등

### 탐색적 시계열 분석

- 이동 평균 평활법(Moving Average Smoothing)

    - 가장 최근의 m-기간 동안의 자료들의 단순 평균을 다음 기간의 예측값으로 추정
    
    - $Z_n$을 시점 n에서의 실제값 $\widehat{Z}_{n+1}$을 시점 n에서 추정한 시점 n + 1의 예측값이라고 하면, $$ \widehat{Z}_{n+1} = \frac{1}{m}(Z_n, Z_{n-1}, \dots + Z_{n-m+1}) $$

- 지수평활법(Exopnential Smoothing)

    - 이동평균평활법과 같이 과거의 관측값을 활용하여 다음 기간의 값을 예측하지만, 최근의 자료에 더 많은 가중치를 부여하는 방법
    
    - $Z_n$을 시점 n에서의 실제값 $S_n$을 시점 지수평활예측값($\widehat{Z}_{n+1}$)이라고 하고, 평활상수 $\alpha$는 0~1 사이의 값이라고 할 때, $$ S_n = \alpha Z_n + (1 - \alpha)S_{n-1} <-> S_n = \alpha Z_n + \alpha(1 - \alpha)Z_{n-1} + \alpha(1 - \alpha)^2Z_{n-2}+\dots $$

- 분해법(추세요인, 계절요인)

    - 변동 요인의 분해 과정
    
        - 원계열 $Z_t$에 추세선(예. 단순선형회귀)을 적합하여 추세성 $\widehat{T}_t$을 추정. $$ Z_t -> \widehat{T}_t = \widehat{\alpha} + \widehat{\beta}t
        
        - 원계열 $Z_t$와 추세성 $\widehat{Z}_t$의 비율 $\widehat{S}_t$을 정의 $$ \widehat{S}_t = \frac{Z_t}{\widehat{T}_t} $$
        
        - k번째 계절에 대한 계절지수 $ \widehat{SI}_k$는 해당 계절의 $\widehat{S}_t$들의 평균으로 구함
    
    - 예측: T시점의 예측값 $\widehat{Z}_T$는 $\widehat{Z}_T = (\widehat{\alpha} + \widehat{\beta}T)\widehat{SI}_K$ ($\widehat{SI}_K$: T시점이 K번째 계절이라고 할 때의 계절지수)

### 확률적 시계열 분석

- 정상 시계열 모형

    - 정상성(stationarity)의 조건
    
        - 평균이 시점에 의존하지 않음
        
        - 분산이 시점에 의존하지 않음
        
        - 공분산은 단지 시차에만 의존하고, 시점 자체에는 의존하지 않음
    
    - 정상성 조건으로 하나라도 만족하지 못하는 시계열 자료를 비정상 시계열이라고 부르며, 비정상 시계열은 정상성을 만족하는 정상시계열 자료로 만든후 자기회귀(AR), 이동평균(MA), 자기회귀이동평균(ARMA) 모형을 적합하는 시계열 분석을 수행한다.
    
    - 자기회귀(AR), 이동평균(MA) 모형의 식별을 위해 자기상관 및 부분자기상관함수의 형태를 이용한다.
    
        - 자기상관계수(autocorrelation, AC): 일정한 간격으로 떨어진 두 시점에서의 시계열 자료 간의 선형적 연관성
        
        - 부분자기상관계수(partial autocorrelation, PAC): 일정한 간격으로 떨어진 두 시점 사이의 상관관계 분석 시, 중간 시점의 영향을 제외하고 측정하는 선형적 연관성