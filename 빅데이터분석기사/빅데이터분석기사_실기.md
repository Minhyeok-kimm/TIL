# 빅데이터 분석기사 실기

- 윈도우 기본 메모장 사용 가능.

- 사용할 수 있는 라이브러리가 제한되어 있음

## 유형별 문제

- 작업 1유형: 처리한 결과값을 print로 출력하고, 답안은 int로 출력해야 한다(다른 불필요한 데이터가 추가되면 감점처리)

- 작업 2유형
  - 예측값을 csv 파일로 제출.
  - 예측값 컬럼 1개만 생성하고 컬럼명은 주어지는 컬럼명을 사용해야 한다. 
  - 자동으로 생성되는 index는 제거해야 한다.
  - 파일 명은 주어지는 파일 명을 사용하며, 별도 디렉토리 지정은 금지한다.

- 작업 3유형
  - 각 문항별로 코딩 화면에서 문제를 풀이한 후 별도의 답안제출 화면으로 이동하여 각 문항별 소문항의 순서대로 답안을 제출. 지시된 제출 형식을 준수한다.

</br>

## 기존 1, 2유형에서 사용할 수 있는 코드들 정리

- 라이브러리 불러오기

```python
import <라이브러리> as <별칭>

import pandas as pd
import numpy as np
```

- 파일 읽기(디렉토리, 파일명은 주어진다.)

```python
import pandas as pd

pd.read_csv('디렉토리/파일명.csv')
pd.read_excel('디렉토리/파일명.csv')
```

- 파일 쓰기(2유형에서 index 컬럼은 제거해야 하기 때문에 index=False로 작성했다.)

```python
df.to_csv('파일이름.csv', index=False)
```

- DataFrame 구조, 정보 확인

```python
df.shape
df.info()
```

- DataFrame 상위/하위 행 출력

```python
df.head(default=5)
df.tail(default=5)
```

- DataFrame index/column 구성요소 확인

```python
df.index
df.columns
```

- Data type 확인 및 변경

```python
<df/series>.dtype
series.astype('변경할 타입')
```

- 데이터 값 변경: regex=True의 경우 일부 값을 대상으로 변경할 수 있다.

```python
series.replace([변경대상1, 변경대상2, ...], [변경내용1, 변경내용2, ...], regex=True/False)
```

- Data 값별 개수 확인
  - 시리즈에서 normalize=True를 주면 비율을 반환한다.
    
```python
series.value_counts()
df.value_counts('column')
```

- 정렬
  - ascending=True: 오름차순(default), ascending=False: 내림차순

```python
df/series.sort_index(ascending=True/False)
df.sort_values(['졍렬행1', '정렬행2',...], ascending=True/False)
```

- Boolean indexing

```python
df.loc[조건, 'column']
```

- str Accessor

```python
series.str.contains('찾을 문자') # True/False로 반환
series.str.upper() # 대문자 반환
series.str.lower() # 소문자 반환
```

- 통계값

```python
df.describe() # 숫자형 데이터의 통계값 반환
series.count() # 개수
series.sum() # 합
series.comsum() # 누적합
series.mean() #평균
series.median() # 중앙값
series.max() # 최댓값
series.min() # 최솟값
series.var() # 분산
series.std() # 표준편차
series.mode() # 최빈값
series.quantile(['수치(비율']) # 비율에 해당하는 값 반환. 주로 IQR(Q3-Q1) 계산에 사용
```

- 그룹별 통계치 확인

```python
df.groupby('column').통계함수()
df.groupby('column').['column'].agg([통계함수1, 통계함수2, ...])
df.pivot_table(index='행방향 column', columns='열방향 column', values='집계 대상 컬럼', aggfunc = '구할 통계값 함수')
```

- 불러올 파일 값 중 'Na'가 포함될 경우 사용 코드

```python
pd.read_csv('디렉토리/파일이름', na_filter=False)
```

- 결측치 채우는 법

```python
df.fillna(값)
series.fillna(값)
df.loc[series.isna(), column] = 값
```

- 컬럼별 함수 적용: series의 통계함수는 문자열 형태로 사용 가능하다.

```python
df.apply(함수)
df.apply([함수1, 함수2, ...])
```

- DataFrame 행/열 제거

```python
df.drop([row1, row2, ...]) # 행 제거. 행 하나 지정시 대괄호 사용하지 않아도 된다.
df.drop(rows=[row]) # 행 제거

df.drop([column1, column2, ...], axis=1) # 열 제거. 열 하나 지정시 대괄호 사용하지 않아도 된다.
df.drop(columns=[column]) # 열 제거
```

- column, index 상호 변경

```python
df.set_index(column) # 해당 column을 index로 지정
df.set_index([column1, column2, ..]) # multi index
df.reset_index() # 모든 index가 column으로 이동, index는 rangeindex로 대체
df.T # 행/열 반전
```

- DataFrame 임의 위치에 열 삽입

```python
df.insert(열번호, 컬럼명, 값) # 기본적으로 inplace=True
```

- DataFrame 합치기

```python
df1.append(df2)
pd.concat([df1, df2, ...], axis=0/1, ignore_index=False/True)
# axis=0: index 방향으로 합치기, axis=1: column 방향으로 합치기
```

- 날짜/시간 타입 변경

  - format: %Y(4자리 년), %y(2자리 년), %m(2자리 월), %d(2자리 일)
  
```python
pd.to_datetime(series, format)
```

- 결측치 제거

  - 아래 코드들은 default 값을 써두었음
  
  - axis=0: 행 방향 동작, axis=1: 열 방향 동작

  - how='any': 결측치가 하나라도 포함된 행 삭제, how='all': 모든 데이터가 결측치인 행 삭제

  - thresh=(int): 해당 숫자 이상의 데이터를 가진 행은 삭제하지 않는다.

  - subset=[column1, column2, ...]: subset으로 지정된 column만 사용해서 삭제 대상을 검색한다.

```python
df.dropna(axis=0, how='any', thresh=None, subset='None', inplace=False)
```

- 결측치 대체

  - mask는 isna()와, where는 notna()와 사용된다.

```python
series.mask(조건, 조건이 참일 때 사용할 값 또는 값 목록)
series.where(조건, 조건이 거짓일 때 사용할 값 또는 값 목록)
```

- 최댓값, 최솟값의 index 찾기

```python
series.argmax() # 가장 큰 값의 integer index
series.argmin() # 가장 작은 값의 integer index
```

- pandas와 numpy의 차이

  - pandas: ddof(자유도)=1 -> 표본에 대한 정보를 나타낸다.

  - numpy: ddof(자유도)=0 -> 모집단에 대한 정보를 나타낸다.

- 표본 추출

1. 단순 무작위 추출

    ```python
    # random_state는 seed값을 주는 것과 같음
    df.sample(frac=비율, random_state=None) # 추출할 비율을 지정.
    df.sample(n=int, random_state=None) # 추출할 개수를 지정한다.
    ```

2. 계통 추출: 특정 step마다 표본을 추출하는 방법

    ```python
    df.iloc[range(start, len(df), step), :]
    ```

3. 층화 추출: 모집단을 몇 개의 집단(층)으로 나눈 후 각 층마다 표본을 추출하는 방법

    ```python
    sample_n=int
    series.value_counts(normalize=True).to_frame
    # 이 후 각각 value 값으로 분리 후 나온 비율로 계산하여 추출
    df = pd.concat([df1, df2])
    ```

- 이상치 탐색

1. boxplot

    ```python
    series/df.plot(kind=box)
    series/df.plot.box()
    ```

2. ESD(평균으로부터 3표준편차 떨어진 값을 이상치로 판단)

    ```python
    e_lower = series.mean() - 3 * series.std
    e_upper = series.mean() + 3 * series.std
    ```

3. 사분위수를 이요한 계산(IQR = Q3 - Q1, Q1 - 1.5IQR 미만, Q3 + 1.5IQR 초과하는 값을 이상치로 판단)

    ```python
    Q1, Q3 = series.quantile([0.25, 0.75])
    
    IQR = Q3- Q1
    
    q_lower = Q1 - 1.5IQR
    
    q_upper = Q3 + 1.5IQR
    ```

- 이상치 처리 방법

1. 정상범주의 데이터만 인덱싱

    ```python
    df.loc[(q_lower >= series)|(q_upper <= series), :]
    ```

2. 이상치 대체

  - `loc[...] = value`의 방식으로 변경

  - numpy 라이브러리에서 올림, 내림, 버림

  - 올림: `np.ceil()`

  - 내림: `np.floor()`

  - 버림: `np.trunc()`

- 데이터 스케일링

    1. Min-Max normalization

        - 값의 범위를 [0, 1]로 변환.

        - `(xi - x.min()) / (x.max() - x.min())`

    2. Standardization

        - 특성의 값이 정규분포를 가지도록 변환 N(0, 1)

        - `(xi - x.mean()) / x.std()`

    - 두 가지 모두 sklearn의 함수로 사용할 수 있다.

    ```python
    from sklearn import MinMaxScaler, StandardScaler
    MinMaxScaler().fit_transform(df)
    StandardScaler().fit_transform(df)
    ```

- Encoding

    1. Label Encoding

        - 값의 일련번호로 표시, 순서가 존재한다.

        - category 타입의 cat.codes(), series의 replace를 사용

    2. One Hot Encoding

        - 범주의 개수만큼 feature를 만드는 방법, 순서가 존재하지 않는다.

        - pd.get_dummies(series/df)

- drop_duplicates: 중복 제거. keep = first / last에 따라 남는 값이 달라진다.

```python
df.drop_duplicates(subset='해당컬럼', keep=first/last)
```

- datetime64 형식에서 특정 일자 및 시간 인덱싱: str 형식으로 반환받은 후 비교 연산자 사용

```python
df['datetime'].dt.strftime(format)
```

---

## 작업 2유형

- 진행 순서: 데이터 가져오기 - 데이터 탐색 - 데이터 타입 확인 - 결측치 확인 - Encoding - (Feature Engineering <-> 학습 모델 생성 및 학습 <-> 성능평가 )- 제출 파일 작성

- dir(), help() 사용 잘 하기

- dtype이 읽어온 데이터와 동일한지 확인 후 변환이 필요할 경우 변환한다.(astype 이용/불필요한 문자 제거)

- 머신러닝 기본 인터페이스

```python
model = 모델함수()
model.fit(x_trian, y_train) # 모델 학습/훈련
score(x_test, y_test) # 성능 측정
predict(x_test) # 예측값 반환
predict_proba(x_test) # 예측확률 반환
```

- 예측값 저장시 사용하는 코드

```python
target = pd.DataFrame()
target['지정컬럼명'] = 학습모델명.predict_proba(X_test)[:, 해당예측숫자]
target.to_csv['파일명.csv', index=False]
```

- Classification

    1. Logistic 회귀분석

        - `from sklearn.linera_model import LogisticRegression`

    2. KNeighborsClassfier

        - `from sklearn.neighbors import KNeighborsClassfier`

    3. DecisionTreeClassfier

        - `from sklearn.tree import DecisionTreeClassfier`

    4. RandomForestClassfier

        - `from sklearn.ensemble import RandomForestClassfier`

    5. XGBClassfier

        - 앙상블의 부스팅 기법. 이전 모델의 오류를 보환하는 방식으로 모델 형성

        - import xgboost import XGBClassfier

- Regression

    1. Logistic 

        - `from sklearn.linear_model import LogisticRegression`

    2. Ridge

        - `from sklearn.linear_model import Ridge`

    3. Lasso

        - `from sklearn.linear_model import Lasso`

    4. DicisionTreeRegressor

        - `from sklearn.tree import DecisionTreeRegressor`

    5. RandomForestRegressor

        - `from sklearn ensemble import RandomForestRegressor`

- 모델 점수 확인

```python
roc_auc_score(y_test, y_pred)
model.score(x_test, y_test)
```

## 작업 3유형

- 통계적 검정

1. 통계적 검정용 모듈: SciPy 모듈

2. 확률분포

    - 분포: 일정한 범위 안에 흩어져 퍼져 있는 정도

    - 확률변수: random variable, 확률 현상에 기인해 결과값이 확률적으로 정해지는 변수</br>확률현상: 어떤 결과들이 나올지 알지만, 가능한 결과 중 어떤 결과가 나올지 모르는 현상

    - 확률분포: 어떤 확률변수가 취할 수 있는 모든 값들과 그 값을 취할 확률의 대응관계로 표시하는 것

3. 이산형 확률분포: 확률변수가 몇 개의 한정된 가능한 값을 가지는 분포. 각 사건은 서로 독립이어야 한다.</br>예) 이항분포, 베르누이분포, 기하분포, 초기하분포, 포아송분포 등

    - 베르누이 분포

        - 매 시행마다 오직 두 가지의 가능한 결과만 일어난다고 할 때, 이러한 실험을 1회 시행하여 일어난 두 가지 결과에 의해 값이 각각 0과 1로 결정하며, 다음을 만족하는 확률변수 X가 따르는 확률분포</br>P(X=1) = p, P(X=0)=q, 0 <= p <= 1, q = 1-p

        - 베르누이 분포의 모수는 p, 기댓값은 E(X) = p, 분산은 Var(X) = pq이다.

    - 기하분포: 베르누이 시행에서 처음 성공까지 시도한 횟수 X의 분포.

        - 성공확률 p인 베르누이 시행에 대해 x번 시행 후 첫 번째 성공을 얻을 확률. X ~ Geom(p)로 표기

        - 계속 실패하다가 x번째에서 성공할 확률을 구할 때 사용함: 문제에서 성공확률, 실패확률 및 실패횟수를 파악해야 함

        - 생성법
        
        ```python
        from scipy.stats import geom # 기하분포 객체 생성함수 import
        geom(p=c)                    # 확률 p=c인 기하확률분포 객체 생성
        y1 = geom(p=c).pmf(x)        # 기하분포의 pmf(특정 횟수 때) 사용, x:확률변수 값, p:확률
        
        geom(0.1).pmf(5)             # 5번째 성공 시 예시
        
        y2 = geom(p=c).cdf(x)        # 기하분포의 cdf(특정 횟수 이내, 이상) 사용, x:확률변수 값, p:확률
        
        geom(0.1).cdf(5)             # 5번 이내 성공 예시
        1-geom(0.1).cdf(4)           # 5번 이상에서 성공 예시
        ```

    - 초기하분포: 비복원추출로 매 실험 조건이 달라지는 경우 사용하는 확률분포(이항분포는 '복원추출'을 사용)

        - M개의 구성원으로 이루어진 전체 모집단, 이 모집단이 두 그룹으로 나누어질 때 첫 번째 그룹에 n개의 구성원이 있고, 다른 그룹에는 M-n개의 구성원이 존재.

            - 이 상황에서 초기하 확률변수 X는 전체 모집단(M)에서 N개의 샘플을 비복원추출 할 때, N개 샘플 중에서 첫 번째 그룹에 해당하는 샘플 수를 의미한다.

        - M개 중 N번 '비복원추출'했을 때 원하는 것 k개가 뽑힐 확률의 분포

        - 생성법

        ```python
        from scipy.stats import hypergeom       # 기하분포 객체 생성 함수 import
        hypergeom(M=100, n=30, N=20)            # 초기하 분포 객체 생성(모집단(M), 성공요소의 개수(n), 시행횟수(N))
        y1 = hypergeom(M=a, n=b, N=c).pmf(x)    # 초기하분포의 pmf(x=N개 중 성공요소의 수)
        y1 = hypergeom(M=a, n=b, N=c).cdf(x)    # 초기하분포의 cdf(x=N개 중 성공요소의 수)
        ```

    - 포아송분포: 단위 시간이나 단위 공간에서 어떤 사건이 몇 번 발생할 것인지를 표현하는 분포

        - 단위 시간 또는 단위 공간 내에서 발생한느 사건의 발생횟수에 따른 확률을 구할 때 사용한다.

        - 확률을 구하기 위해 사건의 평균(λ)과 발생횟수(X)를 알아야 한다.

        - 단위에 대한 부분을 주의해야 한다.(맞춰줘야 함: 분이면 분, 일이면 일 등)

        - 생성법

        ```python
        from scipy.stats import poisson # 포아송 객체 생성 함수 import
        poisson(mu=2)                   # 포아송분포 객체 생성, mu=평균사건발생횟수
        y1 = poissom(mu=2).pmf(x)       # 포아송분포의 pmf. x=확률변수 값(단위시간/공간에서의 사건 발생 횟수)
        y1 = poissom(mu=2).cdf(x)       # 포아송분포의 cdf. x=확률변수 값(단위시간/공간에서의 사건 발생 횟수)
        ```

4. 연속형 확률분포: 확률변수의 가능한 값이 무한 개이며 사실상 셀 수 없을 때</br>예) 정규분포, 연속균일분포, 지수분포, 감마분포, 카이제곱분포, F분포 등

    - 정규분포: 가우스분포라고도 하며, 평균과 표준편차(σ)에 대해 모양이 결정되고, N(평균, 분산)로 표기함

        - 평균 0, 표준편차/분산 1인 정규 분포 N(0, 1)을 표준 정규분포, Z 분포라고 함

        - 평균 주위로 값들이 표준편차의 1배 범위 안에 있을 확률 0.68, 2배 범위 안 0.95, 3배 범위 안 0.997이다.

        - 생성법

        ```python
        from scipy.stats import norm                                # 정규분포 객체 생성함수 import
        norm(loc=0, sclae=1)                                        # 정규분포 객체 생성. loc=평균, scale=표준편차
        
        y1 = norm(loc=mu, scale = std).pdf(x)                       # pdf 사용. x=확률변수값(평균)/배열도 가능
        y1 = norm(loc=mu, scale = std).cdf(x)                       # cdf 사용. x=확률변수값(평균)/배열도 가능
        y1 = norm(loc=mu, scale = stderr).ppf(confidence)           # ppf 사용(표본오차, 신뢰도 등을 구할 때 사용. 임계값을 구하는데 사용.). confidence: 확률값, 신뢰도(계산 필요), scale=표준오차
        r1, r2 = norm.interval(confidence, loc=mu, scale=stderr)    # interval 사용. confidence=신뢰도. scale=표준오차.
        ```

        - 표준오차는 stats.sem(data)를 사용해 구할 수 있다(=표준편차/n ** 0.5)

        - cdf와 ppf는 입출력이 반대 개념이다.

        - ppf에서 신뢰도 95%일 때 confidence는 [0.025, 0.975]를 입력

        - interval은 신뢰구간을 구하는 메서드. 신뢰도 95%일 때 confidence는 0.95를 넣는다.

        - 분산이 주어지면 분산 ** 0.5를 하면 표준편차.

- 가설 검정

    - 가설 검정: 모집단에 대해 가설 설정 후 표본관찰을 통해 그 가설의 채택 여부를 결정하는 통계적 추론 방법

        - 절차

            1. 가설 설정: 귀무가설/대립가설

            2. 유의수준(a) 설정: 제 1종 오류의 최대 허용 한계

            3. 검정통계량 산출: 검정통계량을 통해 p-value 산출

            4. 기각/채택 판단: p-value < a 일 때 귀무가설 기각

    - 가설의 종류

        - 귀무가설: 가설검정의 대상이 되는 가설, 연구자가 부정하고자 하는 가설. </br>알고 있는 것과 같음, 변화 없음, 영향력 없음, 연관성 없음, 효과 없음에 대한 가설

        - 대립가설(=연구가설): 연구자가 연구를 통해 입증/증명되기를 기대하는 예상이나 주장 </br> 귀무가설이 기각되면 채택되는 가설 </br> 알고 있는 것과 다름, 변화 있음, 영향력 있음, 연관성 있음, 효과 있음에 대한 가설

    - 가설검정의 종류: 귀무가설을 기각하는 영역의 위치에 따라 세 가지로 나눔

        1. 양측검정: 귀무가설을 기각하는 영역이 양쪽에 있는 검정

        2. 단측검정(좌측 검정, 우측 검정): 귀무가설을 기각하는 영역이 좌측/우측에 있는 검정

    - 귀무가설과 대립가설의 설정

        - 귀무가설은 "같다, 이상, 이하"의 세 가지 유형이 있다. <-> 대립가설은 "다르다, 작다(미만), 크다(초과)"의 세 가지 유형이 있다.

        - 귀무가설 및 대립가설은 모집단의 모수에 대한 추론으로 평균, 분산, 확률을 가설에 사용한다.

        ex) 휴대푠 평균 교체주기가 2년이다(귀무가설) <-> 휴대폰 평균 교체주기가 2년이 아니다(대립가설)
        </br> 분산이 7보다 크거나 같다(귀무가설) <-> 분산이 7보다 작다(대립가설)
        </br> 제품 분실률은 5% 이하이다(귀무가설) <-> 제품 분실률은 5%보다 크다(대립가설)

    - 제 1종/2종 오류

        - 제 1종 오류(a error): 귀무가설이 참인데 기각되는 오류.

        - 제 2종 오류(b error): 귀무가설이 거짓인데 채택하는 오류.

        - 제1종 오류를 범할 확률의 최대 혀용치를 특정값(유의수준)으로 지정해 놓고, 제 2종 오류의 확률을 가장 작게 해주는 검정 방법을 사용한다.

        - 신뢰수준(1-a): 1종 오류를 범하지 않을 확률. 연구활동(99%), 일반적(95%), 단순설문조사(90%) 사용

        - 검정력(1-b): 2종 오류를 범하지 않을 확률

        - 기각역: 귀무가설을 기각하고 대립가설을 채택하게 되는 영역

        - 유의수준: 귀무가설이 참인데도 기각시키는 확률(제1종 오류 발생 확률)의 최대 허용 한계
        </br> 유의수준 0.05(5%) = 100번 실험에서 제1종 오류를 범하는 최대 허용 한계가 5번
        </br> 유의수준 = 1 - 신뢰수준, 유의수준 = 기각역의 총합

    - 검정통계량: 귀무가설을 채택할지 기각할지를 정하는 판단 기준이 되는 통계량(그래프 x축 값)

        - 분포표를 사용해 자유도를 고려한 검정통계량, 확률값(p-value)을 구할 수 있음

    - 유의확률(p-value): 자유도를 고려한 검정통계량에 관한 확률.

        - 귀무가설의 신뢰구간을 벗어나는 확률(=극단적인 표본 값이 나올 확률), 판정이 잘못되었을 확률

        - 제1종 오류를 범할 확률, 귀무가설을 지지하는 정도

        - p-value가 작을 수록 그 정도가 약하다고 보며, p-value < a 일 때, 귀무가설을 기각, 대립가설을 채택

        - p-value가 0.05 : 귀무가설을 기각했을 때 기각 결정이 잘못될 확률이 5%

        - 검정통계량의 절대값이 임계값보다 큰 경우 귀무가설 기각. / 유의 확률이 유의수준보다 작으면 귀무가설 기각.

- 정규성 검정: 데이터셋의 분포가 정규분포를 따르는지 검정

    - 중심극한 정리에 의해 표본 크기가 30보다 크면 표본평균의 분포는 모집단의 분포 모양과는 관계없이 정규분포에 가까워짐.

    - 30이 넘어도 데이터 특이성에 따라 따르지 않을 수 있기 때문에 데이터의 정규분포를 확인하는 것이 좋다.

    - 종류 및 가설

        - 종류

        1. Shapiro-Wilks Test: 표본수가 5000 미만인 데이터셋에 적합한 정규성 검정. p-value 반환

        2. Kolmogorov-Smirnov Test: 표본수가 2000 초과인 데이터셋에 적합한 정규성 검정. p-value 반환

        3. Normal Test: 왜도와 첨도를 통해 정규성 검정, 20개 이상의 데이터 필요, p-value 반환

        4. Anderson Darling Test: 5개 유의수준([15. 10. 5. 2.5 1.])에 대한 임계값 반환

        - 가설

            - 귀무가설: 데이터셋이 정규분포를 따른다.(p-value > 유의수준, 검정통계량 < 임계값)

            - 대립가설: 데이터셋이 정규분포르 따르지 않는다.

    - 정규성 검정 관련 함수

    ```python
    from scipy.stats import shapiro, kstest, normaltest, anderson
    
    shapiro(x) # statistic(통계량), p-value 반환
    
    kstest(x, "norm") # 두 분포가 같은지 다른지 비교. statistic(통계량), p-value 반환
    kstest(x, stats.norm.cdf) # alternative='two-sided'(양측), 'less'(좌측), 'greater'(우측)
    
    normaltest(x) # statistic(통계량), p-value 반환
    
    anderson(x, dist="norm") # statistic, critical_values(임계값), significance_level(유의수준 반환)
    # dist={'norm', 'expon', 'logistic', 'gumbel_l', 'gumbel_r', 'extreme1'}
    ```

- 등분산성 검정: 분산이 동일한지 검정

    - scipy.stats에서 이를 위한 bartlett-killen, levene, fligner 등이 있음

    - 둘 이상의 정규성을 만족하는 데이터 집합에 대해 모분산이 같은 지 확인하기 위한 검정에는 bartlett 사용

    - 정규성을 만족하지 않는 경우 levene, fligner를 사용

    - 종류 및 가설

        - 종류

            - Bartlett Killen Test: 정규성을 충족하는, 데이터셋의 크기가 서로 다른 2개 이상의 집단에 사용 가능

            - Levene Test: 정규성을 충족하지 않는 비모수 데이터 사용 가능(중앙을 median으로 설정)
            </br> 2개 이상의 집단 사용 가능

            - Fligner Test: Levene Test와 동일한 특성, 비모수 데이터에 더 강건하게 검정이 가능하다.

        - 가설

            - 귀무가설: 데이터셋이 등분산성을 충족한다.

            - 대립가설: 데이터셋이 등분산성을 충족하지 않는다.

    - 함수

    ```python
    from scipy.stats import bartlett, levene, fligner
    
    bartlett(*samples) # statistic, p-value 반환
    
    levene(*samples, center='median', proportiontocut=0.05) # statistic, p-value 반환
    
    fligner(*samples, center='median', proportiontocut=0.05) # statistic, p-value 반환
    
    # center={'median', 'mean', 'trimmed'} / proportiontocut은 trimmed일 경우 사용
    ```

- T-test(=스튜던트 t-테스트): 검정 통계량이 귀무가설 하에서 t-분포를 따르는 통계적 가설 검정

    - 표본을 사용한 모평균 검정 및 두 데이터 세트(집단)의 모평균이 서로 유의하게 다른지 여부를 판단할 수 있음.

    - 검정통계량이 정규분포를 따르며 모집단의 분산, 표준편차를 알지 못할 때 표본으로부터 추정된 분산/표준편차를 사용해 검정

    - 정규성, 등분산성 조건이 만족되어야 한다.

    - 종류 및 가설

        - 종류

            - One Sample t-test: 표본을 사용한 모평균 검정방법
            </br>예) 귀무가설: S사 USB의 평균 수명은 20000시간이다.

            - Paired t-test(대응표본 t-검정)

                - 동일 개체에 어떤 처리를 하기 전, 후의 자료를 얻을 때 차이 값에 대한 평균 검정을 위한 방법
                </br>예: 매일 1시간 한 달 걸으면 2Kg이 빠진다(걷기 수행 전/수행 후)

                - 가능한 동일한 특성을 갖는 두 개체에 서로 다른 처리를 하여 그 처리의 효과를 비교하는 방법
                </br>예: X질병 환자들을 두 집단으로 나누어 A, B 약을 투약해 약의 효과 비교

            - Two sample t-test(독립표본 t-검정): 서로 다른 두 그룹의 표본 평균을 비교하여 두 모집단의 평균 차이가 있는지 검정하는 방법
            </br>예) 귀무가설: 두 집단의 평균 차이 값이 0이다. 2학년과 3학년의 결석률은 같다.

        - 가설

            - One Sample t-test

                - two-sided: 귀무가설 - 모집단의 평균이 A와 같다 / 대립가설 - 모집단의 평균이 A와 같지 않다

                - less: 귀무가설 - 모집단의 평균이 A보다 크거나 같다 / 대립가설 - 모집단의 평균이 A보다 작다

                - greater: 귀무가설 - 모집단의 평균이 A보다 작거나 같다 / 대립가설 - 모집단의 평균이 A보다 크다

            - paired t-test, Two Sample t-test

                - two-sided: 귀무가설 - 두 집단 간 평균 차이는 0이다 / 대립가설 - 두 집단 간 평균 차이는 0이 아니다

                - less: 귀무가설 - 두 집단 평균의 차는 0보다 크거나 같다 / 대립가설 - 두 집단 평균의 차는 0보다 작다
                </br> 귀무가설 - A집단 평균이 B집단 평균보다 크거나 같다. / 대립가설 - A집단 평균이 B집단 평균보다 작다

                - greater: 귀무가설 - 두 집단 평균의 차는 0보다 작거나 같다 / 대립가설 - 두 집단 평균의 차는 0보다 크다
                </br> 귀무가설 - A집단 평균이 B집단 평균보다 작거나 같다 / 대립가설 - A집단 평균이 B집단 평균보다 크다

    - 함수

    ```python
    from scipy.stats import ttest_1samp, ttest_rel, ttest_ind
    
    ttest_1samp(a, popmean, alternative='two-sided') # statistic, p-value 반환 / popmean = 모집단의 평균으로 가정한 값
    
    ttest_rel(a, b, alternative='two-sided') # statistic, p-value 반환 / a=처리 후 집단, b=처리 전 집단
    
    ttest_ind(a, b, alternative='two-sided', equal_var=True) # statistic, p-value 반환
    # equal_var=False인 경우 Welch's t-test를 수행
    ```

    - alternative less -> 효과 있음에 대해 판단할 때 사용
    </br> `ttest_rel(df['after'], df['before'], alternative='less')`
